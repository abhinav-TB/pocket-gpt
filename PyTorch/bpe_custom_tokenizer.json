{
  "vocab_size": 1000,
  "merges": {
    "i_n": 0,
    "o_r": 1,
    "e_n": 2,
    "i_s": 3,
    "in_g": 4,
    "t_o": 5,
    "i_t": 6,
    "e_l": 7,
    "r_e": 8,
    "p_l": 9,
    "u_s": 10,
    "r_a": 11,
    "B_P": 12,
    "BP_E": 13,
    "to_k": 14,
    "tok_en": 15,
    "token_i": 16,
    "tokeni_z": 17,
    "w_or": 18,
    "t_i": 19,
    "it_h": 20,
    "c_e": 21,
    "q_u": 22,
    "qu_en": 23,
    "b_y": 24,
    "by_t": 25,
    "byt_e": 26,
    "h_el": 27,
    "hel_l": 28,
    "hell_o": 29,
    "T_h": 30,
    "Th_is": 31,
    "s_a": 32,
    "sa_m": 33,
    "sam_pl": 34,
    "sampl_e": 35,
    "c_or": 36,
    "cor_p": 37,
    "corp_us": 38,
    "f_or": 39,
    "t_ra": 40,
    "tra_in": 41,
    "train_ing": 42,
    "tokeniz_e": 43,
    "tokenize_r": 44,
    "tokenizer_.": 45,
    "s_u": 46,
    "su_b": 47,
    "sub_wor": 48,
    "subwor_d": 49,
    "tokeniz_a": 50,
    "tokeniza_ti": 51,
    "tokenizati_o": 52,
    "tokenizatio_n": 53,
    "a_l": 54,
    "al_g": 55,
    "alg_or": 56,
    "algor_ith": 57,
    "algorith_m": 58,
    "algorithm_.": 59,
    "I_t": 60,
    "it_e": 61,
    "ite_ra": 62,
    "itera_ti": 63,
    "iterati_v": 64,
    "iterativ_el": 65,
    "iterativel_y": 66,
    "re_pl": 67,
    "repl_a": 68,
    "repla_ce": 69,
    "replace_s": 70,
    "t_h": 71,
    "th_e": 72,
    "m_o": 73,
    "mo_s": 74,
    "mos_t": 75,
    "f_re": 76,
    "fre_quen": 77,
    "frequen_t": 78,
    "p_a": 79,
    "pa_i": 80,
    "pai_r": 81,
    "o_f": 82,
    "byte_s": 83,
    "s_e": 84,
    "se_quen": 85,
    "sequen_ce": 86,
    "w_ith": 87,
    "s_ing": 88,
    "sing_l": 89,
    "singl_e": 90,
    "single_,": 91,
    "u_n": 92,
    "un_us": 93,
    "unus_e": 94,
    "unuse_d": 95,
    "byte_.": 96,
    "wor_l": 97,
    "worl_d": 98,
    "h_o": 99,
    "ho_w": 100,
    "a_re": 101,
    "y_o": 102,
    "yo_u": 103,
    "d_o": 104,
    "do_ing": 105,
    "to_d": 106,
    "tod_a": 107,
    "toda_y": 108,
    "a_g": 109,
    "ag_a": 110,
    "aga_in": 111
  },
  "vocab": {
    "<unk>": 0,
    "<pad>": 1,
    "<bos>": 2,
    "<eos>": 3,
    "B": 4,
    "P": 5,
    "BP": 6,
    "E": 7,
    "BPE": 8,
    "I": 9,
    "t": 10,
    "It": 11,
    "T": 12,
    "h": 13,
    "Th": 14,
    "is": 15,
    "This": 16,
    "a": 17,
    "g": 18,
    "ag": 19,
    "l": 20,
    "al": 21,
    "re": 22,
    "are": 23,
    "aga": 24,
    "in": 25,
    "again": 26,
    "alg": 27,
    "or": 28,
    "algor": 29,
    "ith": 30,
    "algorith": 31,
    "m": 32,
    "algorithm": 33,
    ".": 34,
    "algorithm.": 35,
    "b": 36,
    "y": 37,
    "by": 38,
    "byt": 39,
    "e": 40,
    "byte": 41,
    "byte.": 42,
    "s": 43,
    "bytes": 44,
    "c": 45,
    "ce": 46,
    "cor": 47,
    "p": 48,
    "corp": 49,
    "us": 50,
    "corpus": 51,
    "d": 52,
    "o": 53,
    "do": 54,
    "ing": 55,
    "doing": 56,
    "el": 57,
    "n": 58,
    "en": 59,
    "f": 60,
    "for": 61,
    "fre": 62,
    "quen": 63,
    "frequen": 64,
    "frequent": 65,
    "hel": 66,
    "ho": 67,
    "hell": 68,
    "hello": 69,
    "w": 70,
    "how": 71,
    "i": 72,
    "it": 73,
    "ite": 74,
    "ra": 75,
    "itera": 76,
    "ti": 77,
    "iterati": 78,
    "v": 79,
    "iterativ": 80,
    "iterativel": 81,
    "iteratively": 82,
    "mo": 83,
    "mos": 84,
    "most": 85,
    "of": 86,
    "r": 87,
    "pa": 88,
    "pl": 89,
    "pai": 90,
    "pair": 91,
    "q": 92,
    "u": 93,
    "qu": 94,
    "repl": 95,
    "repla": 96,
    "replace": 97,
    "replaces": 98,
    "sa": 99,
    "se": 100,
    "sing": 101,
    "su": 102,
    "sam": 103,
    "sampl": 104,
    "sample": 105,
    "sequen": 106,
    "sequence": 107,
    "singl": 108,
    "single": 109,
    ",": 110,
    "single,": 111,
    "sub": 112,
    "wor": 113,
    "subwor": 114,
    "subword": 115,
    "th": 116,
    "to": 117,
    "tra": 118,
    "the": 119,
    "tod": 120,
    "k": 121,
    "tok": 122,
    "toda": 123,
    "today": 124,
    "token": 125,
    "tokeni": 126,
    "z": 127,
    "tokeniz": 128,
    "tokeniza": 129,
    "tokenize": 130,
    "tokenizati": 131,
    "tokenizatio": 132,
    "tokenization": 133,
    "tokenizer": 134,
    "tokenizer.": 135,
    "train": 136,
    "training": 137,
    "un": 138,
    "unus": 139,
    "unuse": 140,
    "unused": 141,
    "with": 142,
    "worl": 143,
    "world": 144,
    "yo": 145,
    "you": 146
  },
  "special_tokens": [
    "<unk>",
    "<pad>",
    "<bos>",
    "<eos>"
  ]
}